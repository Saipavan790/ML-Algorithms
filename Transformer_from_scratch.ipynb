{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w_3nAB6GiOQr",
        "WftKDX2-XowC",
        "iWwiqju1jnrU",
        "MFJVKU8c_0gn",
        "xJhQ7gvJMb2N",
        "3SZ-BBrJHCvq",
        "NWtwpX825UEg",
        "GNmOdqHw6cGB",
        "ZdcsWKjSR-8H",
        "5KZkq0_yWPNF",
        "A5Y5mXDVXfZq",
        "dL8jWhcJIW8h"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### We'll build Transformer architechure from scratch. To make things easier for us, Let's try to break down the problem into sub problems and try solving these sub problems first. If you are aware of Transformer architechure then you would know that it has 4 components.\n",
        "\n",
        "\n",
        "1.   Embedding Layer (input layer/target input layer)\n",
        "2.   Encoder Layer\n",
        "3.   Decoder Layer\n",
        "4.   Output layer\n",
        "\n"
      ],
      "metadata": {
        "id": "w_3nAB6GiOQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let's try to solve each sub problem first"
      ],
      "metadata": {
        "id": "RMNruguojbbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Alt Text](https://images.datacamp.com/image/upload/v1704797298/image_7b08f474e7.png)"
      ],
      "metadata": {
        "id": "2hJnOZOh3Ti9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Blocks of the Transformer"
      ],
      "metadata": {
        "id": "WftKDX2-XowC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Layer\n",
        "\n",
        "It has two parts\n",
        "\n",
        "\n",
        "1.   Word encoding\n",
        "2.   Positional encoding (adding positional information of each word to the word encoding in the input text)\n",
        "\n"
      ],
      "metadata": {
        "id": "iWwiqju1jnrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "AFooeg98l_Wf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self,source_vocab_size:int,d_model:int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = source_vocab_size\n",
        "    self.embedding = nn.Embedding(source_vocab_size,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.embedding(x)*np.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "x73jrruH3doD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Positional Encoding"
      ],
      "metadata": {
        "id": "E2lRa_yYkMzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros([10,10,5],dtype=torch.float64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pvl7XAhpoIMB",
        "outputId": "fa077f9b-85fd-4b3d-d5c2-a1ff4644d100"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.]]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 10\n",
        "torch.arange(seq_len,dtype=torch.float64).view(seq_len,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVU2oNfzpzln",
        "outputId": "2202298e-f297-4eda-c10d-f2af626a26fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [2.],\n",
              "        [3.],\n",
              "        [4.],\n",
              "        [5.],\n",
              "        [6.],\n",
              "        [7.],\n",
              "        [8.],\n",
              "        [9.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros([1,4]) + 10000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxgfsJKNAIri",
        "outputId": "b6ce939b-44c7-4dba-830e-d5a29ffe2345"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10000., 10000., 10000., 10000.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = torch.arange(0,4,2, dtype=torch.float64)\n",
        "ct = torch.zeros([1,2]) + 10000\n",
        "torch.pow(ct,indices/4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rirPkUma_MeR",
        "outputId": "2b9deca6-f740-4a4c-a23f-bdaccd29f953"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  1., 100.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self,seq_len,d_model):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    PE = torch.zeros([seq_len,d_model],dtype=torch.float64)         ## dim : [seq_len,d_model]\n",
        "    pos = torch.arange(seq_len,dtype=torch.float64).view(seq_len,1) ## dim : [seq_len,1]\n",
        "\n",
        "    indices = torch.arange(0,d_model,2, dtype=torch.float64)\n",
        "    den = torch.pow(10000,indices/d_model)\n",
        "\n",
        "    PE[:,0::2] = torch.sin(pos/den)\n",
        "    PE[:,1::2] = torch.cos(pos/den)\n",
        "\n",
        "    self.register_buffer('PE', PE.unsqueeze(0))\n",
        "\n",
        "\n",
        "  def forward(self,X):\n",
        "\n",
        "    return X + self.PE[:,X.size(1),:]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zLQdvdhikQFa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding(10,512)"
      ],
      "metadata": {
        "id": "340slL04m8aH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn([2,5,512])\n",
        "pe.forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUcbrSRBHQyO",
        "outputId": "53d4f255-cc22-4a64-8e52-d12a489c7972"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-2.0126,  0.6561, -0.6410,  ...,  0.0515, -1.3291,  1.8826],\n",
              "         [ 0.1997,  2.1964, -0.1028,  ...,  0.2910, -1.1956,  0.9919],\n",
              "         [-1.1606, -1.1618, -0.0578,  ...,  2.1948, -0.0598,  0.7719],\n",
              "         [-1.8819,  0.4361, -1.4111,  ...,  0.5100, -0.2765,  1.5825],\n",
              "         [-0.9097,  0.6576, -1.1641,  ...,  0.9307, -0.2033,  2.4023]],\n",
              "\n",
              "        [[-0.8413, -0.2362, -0.1425,  ...,  0.8740, -2.3125,  0.8214],\n",
              "         [-0.6868,  0.1160,  0.9647,  ...,  1.7587,  0.8765, -0.3766],\n",
              "         [-0.6648,  1.2403, -1.2622,  ...,  1.9908, -1.8640,  0.6998],\n",
              "         [-2.2293,  1.9843, -2.3635,  ...,  0.1400, -0.6205,  0.7977],\n",
              "         [-1.3378,  1.8689,  0.1437,  ...,  0.1159,  0.7912,  0.2840]]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalization"
      ],
      "metadata": {
        "id": "MFJVKU8c_0gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(2,10)\n",
        "mu = x.mean(dim=-1,keepdim=True)\n",
        "sigma = x.std(dim=-1, keepdim=True)\n",
        "\n",
        "print('input: ',x)\n",
        "print('mean: ', mu)\n",
        "print('std: ', sigma)\n",
        "\n",
        "print('output: ', (x-mu)/sigma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u7RkTbDC1bF",
        "outputId": "cad8f3b7-361e-4884-805a-7dd4634ec158"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  tensor([[-0.2085, -1.7421, -2.7194,  0.1361, -0.7113, -0.4613,  1.2713,  2.0897,\n",
            "         -1.7922,  0.1273],\n",
            "        [-1.3243,  0.4628,  0.5659, -0.2282,  0.1579,  0.3181,  0.1855, -0.5091,\n",
            "         -0.8930, -0.4898]])\n",
            "mean:  tensor([[-0.4010],\n",
            "        [-0.1754]])\n",
            "std:  tensor([[1.4458],\n",
            "        [0.6227]])\n",
            "output:  tensor([[ 0.1332, -0.9275, -1.6035,  0.3715, -0.2146, -0.0417,  1.1567,  1.7227,\n",
            "         -0.9622,  0.3654],\n",
            "        [-1.8450,  1.0249,  1.1906, -0.0848,  0.5352,  0.7925,  0.5797, -0.5359,\n",
            "         -1.1524, -0.5048]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(1.1379-(0.0440))/0.8895"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnRrPkuMDb94",
        "outputId": "d2067885-cd19-4347-8d19-ba0ed007858e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2297920179876334"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "  def __init__(self, eps:float=10**-6):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "\n",
        "    ## Learned during training\n",
        "    self.alpha = nn.Parameter(torch.ones(1))   ## Multiplied with normalized input\n",
        "    self.beta =  nn.Parameter(torch.zeros(1))  ## Added (bias)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    mu = x.mean(dim=-1, keepdim=True)    ## mean separately for each sample\n",
        "    sigma = x.std(dim=-1, keepdim=True)  ## Std separately for each sample\n",
        "\n",
        "    return self.alpha*((x-mu)/(sigma+self.eps)) + self.beta"
      ],
      "metadata": {
        "id": "hbxtpFBo_4rF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi headed Attention"
      ],
      "metadata": {
        "id": "xJhQ7gvJMb2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self,d_model : int,h : int, dropout: float):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "\n",
        "    assert self.d_model % self.h == 0, \"Input dimension is not divisble by no of heads\"\n",
        "    self.d_k = self.d_model // self.h\n",
        "\n",
        "    self.w_q = nn.Linear(self.d_model,self.d_model)    ## wq shape : (d_model, d_model)\n",
        "    self.w_k = nn.Linear(self.d_model,self.d_model)    ## wk\n",
        "    self.w_v = nn.Linear(self.d_model,self.d_model)    ## wv\n",
        "\n",
        "    self.w_o = nn.Linear(self.d_model,self.d_model)    ## wo\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, values, mask, dropout:nn.Dropout):\n",
        "    d_k = query.shape[3]\n",
        "\n",
        "    attention_scores = torch.matmul(query,key.transpose(-2,-1))/ np.sqrt(d_k)  ## (batch_size, h, seq_len, d_k) -> (batch_size, h, seq_len, seq_len)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return torch.matmul(attention_scores, values), attention_scores\n",
        "\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "\n",
        "    query = self.w_q(q) ## (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
        "    key = self.w_k(k)   ## (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
        "    values = self.w_v(v) ## (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
        "\n",
        "    query = query.view(query.shape[0],query.shape[1],self.h,self.d_k).transpose(1,2)     ## (batch_size, seq_len, d_model) -> (batch_size, seq_len, h, d_k) -> (batch_size, h, seq_len, d_k)\n",
        "    key = key.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)             ## (batch_size, seq_len, d_model) -> (batch_size, seq_len, h, d_k) -> (batch_size, h, seq_len, d_k)\n",
        "    values = values.view(values.shape[0],values.shape[1],self.h,self.d_k).transpose(1,2) ## (batch_size, seq_len, d_model) -> (batch_size, seq_len, h, d_k) -> (batch_size, h, seq_len, d_k)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttention.attention(query, key, values, mask, self.dropout)\n",
        "\n",
        "    x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h*self.d_k)  ## (batch_size, num_heads, seq_len, h) -> (batch_size, seq_len, d_model)\n",
        "\n",
        "    return self.w_o(x)   ## (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n"
      ],
      "metadata": {
        "id": "b3R6r2bhMWJc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = MultiHeadAttention(16,4,0.0)\n",
        "x = torch.randn([1,10,16])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aRftOnkp9Q2",
        "outputId": "2ca7b695-d765-4a7b-d035-2f9cd0c23382"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.8003,  1.3479, -0.4519, -0.1117,  0.6905,  0.4351,  0.5779,\n",
            "          -0.5061, -0.2013,  0.1791,  0.2344,  1.7502, -0.9007, -0.4884,\n",
            "          -0.5131, -0.8985],\n",
            "         [ 0.3860,  1.2616,  0.0962, -0.1063, -0.2013,  0.5360, -1.8349,\n",
            "           1.1832,  1.1993, -0.4601, -0.1860,  0.1926,  1.2660, -1.7652,\n",
            "          -0.6127, -1.5354],\n",
            "         [ 0.6061,  0.6053, -0.9909, -1.6071, -1.6250,  2.4917,  0.8783,\n",
            "          -0.8135, -0.3035,  0.3632,  1.7293, -1.8015, -0.5320, -0.9973,\n",
            "           1.6437, -2.2311],\n",
            "         [ 0.4926,  1.9545, -1.2315, -0.1473, -0.8703,  2.1518, -0.6912,\n",
            "           0.1429, -1.5533,  0.1572, -0.3270, -1.0155,  0.8278, -0.3608,\n",
            "          -2.3322,  0.7624],\n",
            "         [ 0.0512, -0.0764, -0.8885,  0.5825, -1.3498, -1.5043,  1.7731,\n",
            "           1.7117, -1.2215, -2.6605,  0.1961, -0.8728,  0.5006,  0.7561,\n",
            "          -1.0037,  1.1572],\n",
            "         [ 2.3132,  1.3605,  0.3851, -1.3749, -1.2864, -1.5665, -0.7109,\n",
            "          -1.0166,  0.3458, -0.4048,  0.1444,  0.7904, -0.6054,  1.0888,\n",
            "           0.0725,  0.2910],\n",
            "         [ 0.7609, -1.1133,  0.0633, -0.3592,  0.1336, -1.2097,  1.9820,\n",
            "           1.5812,  1.0114,  0.5757,  1.2200,  0.5885, -0.2265,  1.0980,\n",
            "           1.5862,  0.6933],\n",
            "         [-0.3272,  2.3493,  0.2566,  0.3011, -0.5677, -1.5486,  0.7257,\n",
            "           0.4888, -1.0300, -0.2068, -0.8334, -0.3058, -0.1310,  0.9270,\n",
            "           0.7226, -0.3068],\n",
            "         [ 0.3526,  0.5253, -0.2210, -2.0325,  0.0973, -0.5569, -0.5484,\n",
            "           0.1920, -1.3456,  1.3000, -0.8380, -0.3700,  1.6898, -0.1184,\n",
            "           0.9816, -0.7186],\n",
            "         [-1.6567,  1.0110,  1.0253, -0.9128, -2.0674, -1.4208, -0.6567,\n",
            "          -0.8204, -0.3829,  0.5476, -0.8058, -0.4717,  1.6863, -0.8220,\n",
            "          -0.5250, -1.0655]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = attention.forward(x,x,x,None)"
      ],
      "metadata": {
        "id": "8161xkLwqcFR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention.attention_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvgC1kLHrVSL",
        "outputId": "22016633-db5e-4f10-f59f-ec4d2c3fe765"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.0895, 0.1320, 0.0924, 0.0931, 0.1002, 0.0937, 0.0869, 0.1033,\n",
              "           0.0865, 0.1224],\n",
              "          [0.0677, 0.0300, 0.1271, 0.0932, 0.1759, 0.1067, 0.1515, 0.1001,\n",
              "           0.0902, 0.0577],\n",
              "          [0.0909, 0.0580, 0.0610, 0.1305, 0.1847, 0.0916, 0.1121, 0.1032,\n",
              "           0.0681, 0.0998],\n",
              "          [0.0794, 0.0701, 0.0700, 0.0951, 0.1787, 0.1172, 0.1441, 0.1073,\n",
              "           0.0594, 0.0788],\n",
              "          [0.2061, 0.1576, 0.0391, 0.1434, 0.0442, 0.0707, 0.0567, 0.0670,\n",
              "           0.1067, 0.1084],\n",
              "          [0.0841, 0.0616, 0.1384, 0.0874, 0.1118, 0.1102, 0.1258, 0.0998,\n",
              "           0.1090, 0.0719],\n",
              "          [0.1593, 0.1845, 0.0573, 0.1212, 0.0411, 0.0636, 0.0466, 0.0715,\n",
              "           0.1183, 0.1365],\n",
              "          [0.1456, 0.0619, 0.0644, 0.1584, 0.1031, 0.0839, 0.0926, 0.0861,\n",
              "           0.1113, 0.0927],\n",
              "          [0.0950, 0.0915, 0.0650, 0.1171, 0.1460, 0.0944, 0.1015, 0.1046,\n",
              "           0.0710, 0.1138],\n",
              "          [0.0801, 0.0809, 0.0800, 0.1034, 0.1600, 0.0992, 0.1110, 0.1092,\n",
              "           0.0706, 0.1057]],\n",
              "\n",
              "         [[0.1082, 0.1089, 0.0607, 0.0734, 0.0684, 0.1126, 0.0737, 0.1263,\n",
              "           0.1407, 0.1271],\n",
              "          [0.1303, 0.0918, 0.1054, 0.1185, 0.1278, 0.0791, 0.1060, 0.0813,\n",
              "           0.0740, 0.0858],\n",
              "          [0.0950, 0.1213, 0.1070, 0.0995, 0.1111, 0.0867, 0.0926, 0.0849,\n",
              "           0.1029, 0.0990],\n",
              "          [0.1034, 0.1302, 0.1449, 0.1137, 0.0958, 0.0868, 0.0802, 0.0689,\n",
              "           0.0831, 0.0930],\n",
              "          [0.0406, 0.0911, 0.0845, 0.0796, 0.2653, 0.0483, 0.1582, 0.0691,\n",
              "           0.0992, 0.0641],\n",
              "          [0.0587, 0.0908, 0.0890, 0.0896, 0.1994, 0.0648, 0.1461, 0.0844,\n",
              "           0.1008, 0.0764],\n",
              "          [0.0525, 0.1153, 0.1285, 0.0919, 0.1287, 0.0869, 0.1152, 0.0833,\n",
              "           0.1103, 0.0872],\n",
              "          [0.0664, 0.1143, 0.0990, 0.0906, 0.1559, 0.0735, 0.1168, 0.0836,\n",
              "           0.1113, 0.0885],\n",
              "          [0.0668, 0.1054, 0.1653, 0.1119, 0.1105, 0.0929, 0.1087, 0.0749,\n",
              "           0.0822, 0.0814],\n",
              "          [0.1096, 0.0948, 0.0609, 0.0859, 0.1371, 0.0781, 0.1090, 0.1082,\n",
              "           0.1150, 0.1013]],\n",
              "\n",
              "         [[0.0993, 0.1152, 0.1017, 0.0907, 0.0930, 0.0924, 0.1050, 0.0884,\n",
              "           0.1109, 0.1034],\n",
              "          [0.0992, 0.0893, 0.0944, 0.1086, 0.1082, 0.1066, 0.0859, 0.1176,\n",
              "           0.0922, 0.0980],\n",
              "          [0.0925, 0.0761, 0.2866, 0.0603, 0.0621, 0.0360, 0.2225, 0.0392,\n",
              "           0.0587, 0.0660],\n",
              "          [0.0974, 0.0976, 0.0816, 0.1051, 0.1046, 0.1121, 0.0773, 0.1189,\n",
              "           0.1020, 0.1034],\n",
              "          [0.0932, 0.0935, 0.1068, 0.1096, 0.1287, 0.0887, 0.0817, 0.1222,\n",
              "           0.0871, 0.0885],\n",
              "          [0.0852, 0.1226, 0.1012, 0.0921, 0.1168, 0.0804, 0.1476, 0.0694,\n",
              "           0.1053, 0.0794],\n",
              "          [0.0752, 0.1105, 0.1812, 0.0810, 0.1323, 0.0429, 0.1938, 0.0525,\n",
              "           0.0747, 0.0559],\n",
              "          [0.0909, 0.1267, 0.0812, 0.0920, 0.1024, 0.0984, 0.1058, 0.0847,\n",
              "           0.1204, 0.0975],\n",
              "          [0.1071, 0.1138, 0.0998, 0.0804, 0.0674, 0.1002, 0.1249, 0.0716,\n",
              "           0.1190, 0.1159],\n",
              "          [0.1014, 0.0909, 0.0733, 0.1046, 0.0913, 0.1287, 0.0746, 0.1202,\n",
              "           0.1035, 0.1114]],\n",
              "\n",
              "         [[0.1024, 0.1121, 0.1055, 0.1125, 0.1275, 0.1231, 0.0869, 0.0955,\n",
              "           0.0611, 0.0735],\n",
              "          [0.1033, 0.0812, 0.1298, 0.0959, 0.1008, 0.1276, 0.0846, 0.1136,\n",
              "           0.0798, 0.0834],\n",
              "          [0.1029, 0.0421, 0.1869, 0.0527, 0.0598, 0.1348, 0.1202, 0.1077,\n",
              "           0.1322, 0.0605],\n",
              "          [0.1213, 0.0276, 0.1400, 0.0555, 0.0963, 0.1496, 0.1256, 0.1369,\n",
              "           0.0953, 0.0519],\n",
              "          [0.0883, 0.0843, 0.0678, 0.0775, 0.0698, 0.0595, 0.1312, 0.0871,\n",
              "           0.1944, 0.1400],\n",
              "          [0.0962, 0.1190, 0.0997, 0.1028, 0.0962, 0.0963, 0.1003, 0.0902,\n",
              "           0.0999, 0.0994],\n",
              "          [0.1057, 0.0814, 0.0452, 0.0949, 0.1368, 0.0658, 0.1535, 0.0913,\n",
              "           0.1191, 0.1064],\n",
              "          [0.1027, 0.1086, 0.1123, 0.1060, 0.1175, 0.1229, 0.0927, 0.0947,\n",
              "           0.0688, 0.0738],\n",
              "          [0.1001, 0.1235, 0.1268, 0.1110, 0.1303, 0.1478, 0.0800, 0.0840,\n",
              "           0.0445, 0.0523],\n",
              "          [0.0705, 0.2568, 0.1274, 0.1305, 0.0847, 0.1090, 0.0499, 0.0589,\n",
              "           0.0424, 0.0699]]]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed Forward layer"
      ],
      "metadata": {
        "id": "3SZ-BBrJHCvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model:int, d_ff:int ,dropout:float):\n",
        "\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)   ## First linear layer\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)   ## Second linear layer\n",
        "    self.dropout = nn.Dropout(dropout)        ## Dropout layer\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    ## (batch_size, seq_len, d_model) --> (batch_size, seq_len, d_ff) --> (batch_size, seq_len, d_model)\n",
        "\n",
        "    return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
        "\n"
      ],
      "metadata": {
        "id": "dZGHsMLHHFNC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Connection"
      ],
      "metadata": {
        "id": "NWtwpX825UEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "  def __init__(self, dropout: float):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "\n",
        "    return x + self.dropout(self.norm(sublayer(x)))\n"
      ],
      "metadata": {
        "id": "z8rDO3aW5XK4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "GNmOdqHw6cGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, self_attention: MultiHeadAttention, feed_forward: FeedForward, dropout: float):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = self_attention\n",
        "    self.feed_forward = feed_forward\n",
        "\n",
        "    self.residualconnections = nn.ModuleList([ResidualConnection(dropout) for j in range(2)])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "\n",
        "    x = self.residualconnections[0](x, lambda x: self.self_attention(x,x,x,src_mask))\n",
        "    x = self.residualconnections[0](x, self.feed_forward)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "5igvmIei6dux"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "pv-vf_baQjCD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "ZdcsWKjSR-8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, self_attention: MultiHeadAttention, cross_attention: MultiHeadAttention, feed_forward: FeedForward, dropout: float):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = self_attention\n",
        "    self.cross_attention = cross_attention\n",
        "    self.feed_forward = feed_forward\n",
        "\n",
        "    self.residualconnections = nn.ModuleList([ResidualConnection(dropout) for j in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "\n",
        "    x = self.residualconnections[0](x, lambda x: self.self_attention(x, x, x, tgt_mask))\n",
        "    x = self.residualconnections[1](x, lambda x: self.cross_attention(x, encoder_output, encoder_output, src_mask))\n",
        "    x = self.residualconnections[2](x, self.feed_forward)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "IalLVGMQSA4o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "8-uylwDYVGhS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Layer"
      ],
      "metadata": {
        "id": "5KZkq0_yWPNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear = nn.Linear(d_model, vocab_size)  ## maps each token to a word in the vocabulary\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    return torch.log_softmax(self.linear(x), dim=-1)   ## Applying softmax along the last dimension"
      ],
      "metadata": {
        "id": "MZn-OASWWRdN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "A5Y5mXDVXfZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, Input_Embedding: InputEmbedding, encoder: Encoder, Output_Embedding: InputEmbedding, decoder: Decoder, Input_pos: PositionalEncoding, Output_pos: PositionalEncoding, Output_layer: OutputLayer):\n",
        "\n",
        "    super().__init__()\n",
        "    self.Input_Embedding = Input_Embedding\n",
        "    self.encoder = encoder\n",
        "    self.Output_Embedding = Output_Embedding\n",
        "    self.decoder = decoder\n",
        "    self.Input_pos = Input_pos\n",
        "    self.Output_pos = Output_pos\n",
        "    self.Output_layer = Output_layer\n",
        "\n",
        "  def encode(self, x_src, src_mask):\n",
        "\n",
        "    x_src = self.Input_Embedding(x_src)\n",
        "    x_src = self.Input_pos(x_src)\n",
        "\n",
        "    return self.encoder(x_src,src_mask)\n",
        "\n",
        "  def decode(self, x_tgt, enoder_output, src_mask, tgt_mask):\n",
        "\n",
        "    x_tgt = self.Output_Embedding(x_tgt)\n",
        "    x_tgt = self.Output_pos(x_tgt)\n",
        "\n",
        "    return self.decoder(x_tgt, enoder_output, src_mask, tgt_mask)\n",
        "\n",
        "  def Ouput(self, x):\n",
        "\n",
        "    return self.Output_layer(x)\n"
      ],
      "metadata": {
        "id": "Cq9QBS6VXiko"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We have completed building basic blocks of a Transformer architechure. Now Let's try to combine all these. So we are gonna write a function which takes set of parameters required and builds the transformer for us"
      ],
      "metadata": {
        "id": "gGIT-VH0H6c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Transformer"
      ],
      "metadata": {
        "id": "dL8jWhcJIW8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BuildTransformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, N: int = 6, d_model: int = 512, h: int = 8, d_ff: int = 2048, dropout: float = 0.1):\n",
        "\n",
        "\n",
        "\n",
        "  Input_Embedding = InputEmbedding(src_vocab_size, d_model)   ## Create embedding layer for source text\n",
        "  Input_pos = PositionalEncoding(src_seq_len, d_model)        ## Create Postional Encoding for source text\n",
        "\n",
        "  Output_Embedding = InputEmbedding(tgt_vocab_size, d_model)  ## Create embedding layer for target text\n",
        "  Output_pos = PositionalEncoding(tgt_seq_len, d_model)       ## Create Postional Encoding for target text\n",
        "\n",
        "  encoder_blocks = []\n",
        "\n",
        "  for e in range(N):\n",
        "\n",
        "    encoder_self_attention = MultiHeadAttention(d_model, h, dropout)\n",
        "    encoder_feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "    encoder_blocks.append(EncoderBlock(encoder_self_attention, encoder_feed_forward, dropout))\n",
        "\n",
        "  encoder_layers = nn.ModuleList(encoder_blocks)\n",
        "\n",
        "  encoder = Encoder(encoder_layers)   ## Encoder\n",
        "\n",
        "  decoder_blocks = []\n",
        "\n",
        "  for d in range(N):\n",
        "\n",
        "    decoder_self_attention = MultiHeadAttention(d_model, h, dropout)\n",
        "    decoder_cross_attention = MultiHeadAttention(d_model, h, dropout)\n",
        "    decoder_feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "    decoder_blocks.append(DecoderBlock(decoder_self_attention, decoder_cross_attention,decoder_feed_forward, dropout))\n",
        "\n",
        "  decoder_layers = nn.ModuleList(decoder_blocks)\n",
        "\n",
        "  decoder = Decoder(decoder_layers)  ## Decoder\n",
        "\n",
        "  Output_layer = OutputLayer(d_model, tgt_vocab_size)  ## Output layer\n",
        "\n",
        "  transformer = Transformer(Input_Embedding, encoder, Output_Embedding, decoder, Input_pos, Output_pos, Output_layer)  ## create a transformer instance\n",
        "\n",
        "  ## Initialize the parameters of the network\n",
        "\n",
        "  for p in transformer.parameters():\n",
        "\n",
        "    if p.dim() > 1:\n",
        "\n",
        "       nn.init.xavier_uniform(p)\n",
        "\n",
        "  print(transformer)\n",
        "\n",
        "  return transformer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RacWhFGbIZF5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tf = BuildTransformer(10000, 8000, 20, 12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq__-hzoSjsq",
        "outputId": "b6b94a5c-060b-444d-fc35-3cb792b01741"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-e384eb79c081>:48: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(p)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (Input_Embedding): InputEmbedding(\n",
            "    (embedding): Embedding(10000, 512)\n",
            "  )\n",
            "  (encoder): Encoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x EncoderBlock(\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): FeedForward(\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (residualconnections): ModuleList(\n",
            "          (0-1): 2 x ResidualConnection(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (norm): LayerNormalization()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNormalization()\n",
            "  )\n",
            "  (Output_Embedding): InputEmbedding(\n",
            "    (embedding): Embedding(8000, 512)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x DecoderBlock(\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (cross_attention): MultiHeadAttention(\n",
            "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): FeedForward(\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (residualconnections): ModuleList(\n",
            "          (0-2): 3 x ResidualConnection(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (norm): LayerNormalization()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNormalization()\n",
            "  )\n",
            "  (Input_pos): PositionalEncoding()\n",
            "  (Output_pos): PositionalEncoding()\n",
            "  (Output_layer): OutputLayer(\n",
            "    (linear): Linear(in_features=512, out_features=8000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UAmv710SSvfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}